{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45364a57-e2d2-4277-a1da-4350c31a2a35",
   "metadata": {
    "id": "45364a57-e2d2-4277-a1da-4350c31a2a35",
    "outputId": "6f2543a9-b0ae-4963-ec41-f6f739ce4d1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mmlu-college-medicine-af'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## ADJUST THESE FIRST THREE VARIABLES THEN RUN ALL CELLS TO FINE-TUNE AND EVALUATE LLAMA 3 70B INSTRUCT\n",
    "## Code adapted from Unsloth sample: https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing\n",
    "\n",
    "# Dataset to fine-tune on; One of mmlu_college_medicine_{lang}.jsonl or winogrande_train_s_{lang}.jsonl where lang is \"af\", \"en\", \"xh\", or \"zu\" for Afrikaans, English, Xhosa, or Zulu, respectively\n",
    "fine_tuning_path = 'mmlu_college_medicine_af.jsonl'\n",
    "\n",
    "# How many times to run evaluation\n",
    "evaluation_runs = 3\n",
    "\n",
    "# Hugging Face token with WRITE permissions to save the model\n",
    "hf_write_token = \"\"\n",
    "\n",
    "fine_tune_name = fine_tuning_path[:-6].replace('_', '-')  # Can also set to something like \"base\" or \"baseline\" if not planning to fine-tune\n",
    "fine_tuning_path = f'winogrande-mmlu-clinical-za/data/gpt_fine_tuning_datasets/{fine_tuning_path}'\n",
    "fine_tune_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7b27e1-c2e6-4971-8e19-2c78c09c9893",
   "metadata": {
    "id": "fa7b27e1-c2e6-4971-8e19-2c78c09c9893",
    "outputId": "25c7903a-306a-4f95-ee33-9c6da540dcca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001B[0m\u001B[33m\n",
      "\u001B[0mRequirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.0)\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0mLooking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Collecting torch==2.2.0\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.2.0%2Bcu121-cp310-cp310-linux_x86_64.whl (757.3 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m757.3/757.3 MB\u001B[0m \u001B[31m115.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting triton\n",
      "  Downloading https://download.pytorch.org/whl/triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m168.1/168.1 MB\u001B[0m \u001B[31m212.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hCollecting filelock (from torch==2.2.0)\n",
      "  Downloading https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch==2.2.0)\n",
      "  Downloading https://download.pytorch.org/whl/typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Collecting sympy (from torch==2.2.0)\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.7/5.7 MB\u001B[0m \u001B[31m114.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hCollecting networkx (from torch==2.2.0)\n",
      "  Downloading https://download.pytorch.org/whl/networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.6/1.6 MB\u001B[0m \u001B[31m145.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting jinja2 (from torch==2.2.0)\n",
      "  Downloading https://download.pytorch.org/whl/Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m133.2/133.2 kB\u001B[0m \u001B[31m176.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting fsspec (from torch==2.2.0)\n",
      "  Downloading https://download.pytorch.org/whl/fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m170.9/170.9 kB\u001B[0m \u001B[31m14.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m23.7/23.7 MB\u001B[0m \u001B[31m90.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m823.6/823.6 kB\u001B[0m \u001B[31m157.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m14.1/14.1 MB\u001B[0m \u001B[31m71.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m731.7/731.7 MB\u001B[0m \u001B[31m123.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m410.6/410.6 MB\u001B[0m \u001B[31m91.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m121.6/121.6 MB\u001B[0m \u001B[31m85.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.5/56.5 MB\u001B[0m \u001B[31m74.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m124.2/124.2 MB\u001B[0m \u001B[31m64.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m196.0/196.0 MB\u001B[0m \u001B[31m113.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m166.0/166.0 MB\u001B[0m \u001B[31m102.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m99.1/99.1 kB\u001B[0m \u001B[31m161.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting triton\n",
      "  Downloading https://download.pytorch.org/whl/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m167.9/167.9 MB\u001B[0m \u001B[31m142.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvjitlink_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (19.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m19.8/19.8 MB\u001B[0m \u001B[31m138.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hCollecting MarkupSafe>=2.0 (from jinja2->torch==2.2.0)\n",
      "  Downloading https://download.pytorch.org/whl/MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch==2.2.0)\n",
      "  Downloading https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m536.2/536.2 kB\u001B[0m \u001B[31m217.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: mpmath, typing-extensions, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch\n",
      "  Attempting uninstall: mpmath\n",
      "    Found existing installation: mpmath 1.3.0\n",
      "    Uninstalling mpmath-1.3.0:\n",
      "      Successfully uninstalled mpmath-1.3.0\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.1.105\n",
      "    Uninstalling nvidia-nvtx-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.3.101\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.3.101:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.3.101\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.19.3\n",
      "    Uninstalling nvidia-nccl-cu12-2.19.3:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.19.3\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.2.106\n",
      "    Uninstalling nvidia-curand-cu12-10.3.2.106:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n",
      "    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 3.2.1\n",
      "    Uninstalling networkx-3.2.1:\n",
      "      Successfully uninstalled networkx-3.2.1\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 2.1.5\n",
      "    Uninstalling MarkupSafe-2.1.5:\n",
      "      Successfully uninstalled MarkupSafe-2.1.5\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.2.0\n",
      "    Uninstalling fsspec-2024.2.0:\n",
      "      Successfully uninstalled fsspec-2024.2.0\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.13.1\n",
      "    Uninstalling filelock-3.13.1:\n",
      "      Successfully uninstalled filelock-3.13.1\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.2.0\n",
      "    Uninstalling triton-2.2.0:\n",
      "      Successfully uninstalled triton-2.2.0\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n",
      "    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n",
      "    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.3\n",
      "    Uninstalling Jinja2-3.1.3:\n",
      "      Successfully uninstalled Jinja2-3.1.3\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n",
      "    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.0\n",
      "    Uninstalling torch-2.2.0:\n",
      "      Successfully uninstalled torch-2.2.0\n",
      "Successfully installed MarkupSafe-2.1.5 filelock-3.13.1 fsspec-2024.2.0 jinja2-3.1.3 mpmath-1.3.0 networkx-3.2.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.1.105 nvidia-nvtx-cu12-12.1.105 sympy-1.12 torch-2.2.0+cu121 triton-2.2.0 typing-extensions-4.9.0\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001B[0m\u001B[33m\n",
      "\u001B[0mCollecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-9z40zt_n/unsloth_78d7ae2c8bbe4edbb843139f409b7fd1\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-9z40zt_n/unsloth_78d7ae2c8bbe4edbb843139f409b7fd1\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit 8d9bd0ea8bf662618ba96fe7fe3478c5b81d0dff\n",
      "  Installing build dependencies ... \u001B[?25ldone\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\n",
      "\u001B[?25h  Installing backend dependencies ... \u001B[?25ldone\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\n",
      "\u001B[?25hCollecting bitsandbytes (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (23.2)\n",
      "Collecting ninja (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting flash-attn (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading flash_attn-2.5.9.post1.tar.gz (2.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.6/2.6 MB\u001B[0m \u001B[31m8.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (2.2.0+cu121)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (1.26.3)\n",
      "Collecting einops (from flash-attn->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting xformers@ https://download.pytorch.org/whl/cu121/xformers-0.0.24-cp310-cp310-manylinux2014_x86_64.whl (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/xformers-0.0.24-cp310-cp310-manylinux2014_x86_64.whl (218.2 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m218.2/218.2 MB\u001B[0m \u001B[31m103.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting tyro (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading tyro-0.8.4-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting transformers>=4.38.2 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m43.8/43.8 kB\u001B[0m \u001B[31m120.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting datasets>=2.16.0 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading datasets-2.19.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting sentencepiece>=0.2.0 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tqdm (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m57.6/57.6 kB\u001B[0m \u001B[31m254.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (5.9.8)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (0.42.0)\n",
      "Collecting accelerate>=0.26.1 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading accelerate-0.31.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting trl<0.9.0,>=0.7.9 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading trl-0.8.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting peft!=0.11.0,>=0.7.1 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting protobuf<4.0.0 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (6.0.1)\n",
      "Collecting huggingface-hub (from accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting safetensors>=0.3.1 (from accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (3.13.1)\n",
      "Collecting pyarrow>=12.0.0 (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting requests>=2.32.1 (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting xxhash (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (2024.2.0)\n",
      "Collecting aiohttp (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (12.1.105)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.38.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m40.9/40.9 kB\u001B[0m \u001B[31m231.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting tokenizers<0.20,>=0.19 (from transformers>=4.38.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting docstring-parser>=0.14.1 (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting rich>=11.1.0 (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (2024.2.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (2.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (1.3.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-ampere-torch220]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\n",
      "Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m119.8/119.8 MB\u001B[0m \u001B[31m70.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m307.2/307.2 kB\u001B[0m \u001B[31m173.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m309.4/309.4 kB\u001B[0m \u001B[31m183.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading datasets-2.19.2-py3-none-any.whl (542 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m542.1/542.1 kB\u001B[0m \u001B[31m167.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m251.6/251.6 kB\u001B[0m \u001B[31m161.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m105.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.3/1.3 MB\u001B[0m \u001B[31m125.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m78.3/78.3 kB\u001B[0m \u001B[31m148.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m9.1/9.1 MB\u001B[0m \u001B[31m66.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hDownloading trl-0.8.6-py3-none-any.whl (245 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m245.2/245.2 kB\u001B[0m \u001B[31m151.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading tyro-0.8.4-py3-none-any.whl (102 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m102.4/102.4 kB\u001B[0m \u001B[31m150.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m43.2/43.2 kB\u001B[0m \u001B[31m102.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m116.3/116.3 kB\u001B[0m \u001B[31m215.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.2/1.2 MB\u001B[0m \u001B[31m162.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m401.7/401.7 kB\u001B[0m \u001B[31m155.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m40.8/40.8 MB\u001B[0m \u001B[31m61.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0mm\n",
      "\u001B[?25hDownloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m775.1/775.1 kB\u001B[0m \u001B[31m136.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m64.9/64.9 kB\u001B[0m \u001B[31m142.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m240.7/240.7 kB\u001B[0m \u001B[31m165.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.2/1.2 MB\u001B[0m \u001B[31m87.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.6/3.6 MB\u001B[0m \u001B[31m52.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m134.8/134.8 kB\u001B[0m \u001B[31m155.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m13.0/13.0 MB\u001B[0m \u001B[31m58.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m194.1/194.1 kB\u001B[0m \u001B[31m164.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m239.5/239.5 kB\u001B[0m \u001B[31m170.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m87.5/87.5 kB\u001B[0m \u001B[31m246.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m124.3/124.3 kB\u001B[0m \u001B[31m123.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m505.5/505.5 kB\u001B[0m \u001B[31m192.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m345.4/345.4 kB\u001B[0m \u001B[31m171.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m301.6/301.6 kB\u001B[0m \u001B[31m137.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: unsloth, flash-attn\n",
      "  Building wheel for unsloth (pyproject.toml) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for unsloth: filename=unsloth-2024.5-py3-none-any.whl size=109654 sha256=3754a28f75e0550238909e2a8cd34fe28fdd575050c8d46fa91626009b4248e7\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-vu01pum3/wheels/ed/d4/e9/76fb290ee3df0a5fc21ce5c2c788e29e9607a2353d8342fd0d\n",
      "  Building wheel for flash-attn (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for flash-attn: filename=flash_attn-2.5.9.post1-cp310-cp310-linux_x86_64.whl size=120821333 sha256=7bfd5ecaaf20577cd1255eaa90d9008a09050b3408ba6388bcbc5b6144f482d0\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-vu01pum3/wheels/cc/ad/f6/7ccf0238790d6346e9fe622923a76ec218e890d356b9a2754a\n",
      "Successfully built unsloth flash-attn\n",
      "Installing collected packages: sentencepiece, pytz, ninja, xxhash, unsloth, tzdata, tqdm, shtab, safetensors, requests, regex, pyarrow-hotfix, pyarrow, protobuf, multidict, mdurl, frozenlist, einops, docstring-parser, dill, async-timeout, yarl, pandas, multiprocess, markdown-it-py, huggingface-hub, aiosignal, tokenizers, rich, aiohttp, xformers, tyro, transformers, flash-attn, bitsandbytes, accelerate, peft, datasets, trl\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade --force-reinstall --no-cache-dir torch==2.2.0 triton --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install \"unsloth[cu121-ampere-torch220] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install pandas\n",
    "!git clone https://github.com/InstituteforDiseaseModeling/winogrande-mmlu-clinical-za.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567263cd-a208-4bc1-b07c-7c24dbb7bc61",
   "metadata": {
    "id": "567263cd-a208-4bc1-b07c-7c24dbb7bc61"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.version.cuda, torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eabedd6-ea1c-4b87-913e-202536101d62",
   "metadata": {
    "id": "5eabedd6-ea1c-4b87-913e-202536101d62"
   },
   "outputs": [],
   "source": [
    "# Check installation status\n",
    "!nvcc\n",
    "!python -m xformers.info\n",
    "!python -m bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859d612a-2dfb-48c0-9af0-c1dba728c67c",
   "metadata": {
    "id": "859d612a-2dfb-48c0-9af0-c1dba728c67c"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-3-70b-Instruct-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66a3813-aeee-4fa2-8637-4bfc00f13337",
   "metadata": {
    "id": "b66a3813-aeee-4fa2-8637-4bfc00f13337"
   },
   "outputs": [],
   "source": [
    "# Skip running this cell if want to evaluate baseline\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 8,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 42,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e04938-27ff-4ee5-8aa7-2c832cb6f733",
   "metadata": {
    "id": "64e04938-27ff-4ee5-8aa7-2c832cb6f733"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"messages\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "train_df = pd.read_json(fine_tuning_path, lines=True)\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "train_ds = train_ds.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703e8c59-0ee0-4dad-8f82-4e9b72a4a202",
   "metadata": {
    "id": "703e8c59-0ee0-4dad-8f82-4e9b72a4a202"
   },
   "outputs": [],
   "source": [
    "# Skip running this cell if want to evaluate baseline\n",
    "train_ds[5]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeea70e-d158-46cf-8d5e-0339f5de40c9",
   "metadata": {
    "id": "ceeea70e-d158-46cf-8d5e-0339f5de40c9"
   },
   "outputs": [],
   "source": [
    "# Skip running this cell if want to evaluate baseline\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_ds,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 10,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 1,\n",
    "        # gradient_accumulation_steps = 4,\n",
    "        # warmup_steps = 5,\n",
    "        # max_steps = 60,\n",
    "        num_train_epochs = 3,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 42,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f7e84e-50d6-47b4-b46f-eb271eb4b181",
   "metadata": {
    "id": "b3f7e84e-50d6-47b4-b46f-eb271eb4b181"
   },
   "outputs": [],
   "source": [
    "# Skip running this cell if want to evaluate baseline\n",
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4470884-7e9e-4cbc-b6fd-f11f358ed8d3",
   "metadata": {
    "id": "a4470884-7e9e-4cbc-b6fd-f11f358ed8d3"
   },
   "outputs": [],
   "source": [
    "# Skip running this cell if want to evaluate baseline\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710bda5d-abc9-44ca-b295-49c80d84cfee",
   "metadata": {
    "id": "710bda5d-abc9-44ca-b295-49c80d84cfee"
   },
   "outputs": [],
   "source": [
    "# Skip running this cell if want to evaluate baseline\n",
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b347b61-33dd-4e84-b185-46cba34193da",
   "metadata": {
    "id": "7b347b61-33dd-4e84-b185-46cba34193da"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3\",\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 512, use_cache = True, temperature = 0.7, top_p = 0.9)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978ae751-5e7a-4ab9-82de-2425b8eb41e3",
   "metadata": {
    "id": "978ae751-5e7a-4ab9-82de-2425b8eb41e3"
   },
   "outputs": [],
   "source": [
    "model_name = f\"llama3-70b-instruct-{fine_tune_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf82f865-f358-4b67-8ec2-f80e39e8e040",
   "metadata": {
    "id": "bf82f865-f358-4b67-8ec2-f80e39e8e040"
   },
   "outputs": [],
   "source": [
    "# Define inference function that accepts a row in an OpenAI Batch API-formatted JSONL and produces a response\n",
    "def infer(jsonl_row):\n",
    "    messages = jsonl_row['body']['messages']\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=jsonl_row['body']['max_tokens'],\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=jsonl_row['body']['temperature'],\n",
    "        top_p=jsonl_row['body']['top_p'],\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "test_row = {\"custom_id\": \"<|MODEL|>-on-en-mmlu-clinical_knowledge-0-answer-A\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"<|MODEL|>\", \"messages\": [{\"role\": \"user\", \"content\": \"The following are multiple choice questions (with answers) about clinical knowledge.\\n\\nQuestion 1: The energy for all forms of muscle contraction is provided by:\\nA. ATP.\\nB. ADP.\\nC. phosphocreatine.\\nD. oxidative phosphorylation.\\nAnswer: A\\n\\nQuestion 2: What is the difference between a male and a female catheter?\\nA. Male and female catheters are different colours.\\nB. Male catheters are longer than female catheters.\\nC. Male catheters are bigger than female catheters.\\nD. Female catheters are longer than male catheters.\\nAnswer: B\\n\\nQuestion 3: In the assessment of the hand function which of the following is true?\\nA. Abduction of the thumb is supplied by spinal root T2\\nB. Opposition of the thumb by opponens policis is supplied by spinal root T1\\nC. Finger adduction is supplied by the median nerve\\nD. Finger abduction is mediated by the palmar interossei\\nAnswer: B\\n\\nQuestion 4: How many attempts should you make to cannulate a patient before passing the job on to a senior colleague, according to the medical knowledge of 2020?\\nA. 4\\nB. 3\\nC. 2\\nD. 1\\nAnswer: C\\n\\nQuestion 5: Glycolysis is the name given to the pathway involving the conversion of:\\nA. glycogen to glucose-1-phosphate.\\nB. glycogen or glucose to fructose.\\nC. glycogen or glucose to pyruvate or lactate.\\nD. glycogen or glucose to pyruvate or acetyl CoA.\\nAnswer: C\\n\\nNow, given the following question and answer choices, output only the letter corresponding to the correct answer. Do not add any explanation.\\n\\nQuestion: What size of cannula would you use in a patient who needed a rapid blood transfusion (as of 2020 medical knowledge)?\\nA. 18 gauge.\\nB. 20 gauge.\\nC. 22 gauge.\\nD. 24 gauge.\\nAnswer:\\n\"}], \"max_tokens\": 512, \"temperature\": 0.7, \"top_p\": 0.9}}\n",
    "infer(test_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92498632-9d97-48fb-b877-897dd161f86e",
   "metadata": {
    "id": "92498632-9d97-48fb-b877-897dd161f86e"
   },
   "outputs": [],
   "source": [
    "!pip install tqdm\n",
    "!pip install seaborn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca2a327-eb48-454c-87a5-cef032815c46",
   "metadata": {
    "id": "cca2a327-eb48-454c-87a5-cef032815c46"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "for i in range(evaluation_runs):\n",
    "    # Infer on all prompts\n",
    "    generations_map = {}\n",
    "    with open(f'winogrande-mmlu-clinical-za/data/evaluation_batches/gpt_style_batch_evaluation_template.jsonl', 'r') as fp:\n",
    "        all_prompts_jsonl = pd.read_json(fp, lines=True)\n",
    "\n",
    "    for index, row in tqdm(all_prompts_jsonl.iterrows(), total=all_prompts_jsonl.shape[0]):\n",
    "        if '-mmlu-college_medicine-' in row['custom_id']:  # do not evaluate on college medicine since some models were trained on it in the experiments\n",
    "            continue\n",
    "        generations_map[row['custom_id'].replace('<|MODEL|>', model_name)] = infer({'custom_id': row['custom_id'], 'body': row['body']})\n",
    "\n",
    "    # Save generations so they never have to be run again\n",
    "    with open(f'generations_{model_name}_{i}.json', 'w') as fp:\n",
    "        json.dump(generations_map, fp, indent=2)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d204199-2118-4b27-b3ac-5d212f26aaa4",
   "metadata": {
    "id": "7d204199-2118-4b27-b3ac-5d212f26aaa4"
   },
   "outputs": [],
   "source": [
    "# Define response-to-correctness functions\n",
    "\n",
    "def check_mc_answer(custom_id, generation):\n",
    "    parsed_gen = generation.strip().replace('(', ''). replace(')', '').upper()\n",
    "    return len(parsed_gen) > 0 and parsed_gen[0] == custom_id[-1]  # answer is stored in last number of custom_id\n",
    "\n",
    "def check_winogrande_answer(custom_id, generation):\n",
    "    correct_number = custom_id[-1]  # answer is stored in the last character of the custom_id\n",
    "    incorrect_number = str(3 - int(correct_number))  # maps 1 to 2 and 2 to 1\n",
    "    correct = correct_number in generation and incorrect_number not in generation\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a83eed9-e35d-41ba-a38f-32aeb07614cf",
   "metadata": {
    "id": "2a83eed9-e35d-41ba-a38f-32aeb07614cf"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "for i in range(evaluation_runs):\n",
    "\n",
    "    with open(f'generations_{model_name}_{i}.json', 'r') as fp:\n",
    "        generations_map = json.load(fp)\n",
    "\n",
    "    # Get and display MMLU performance\n",
    "\n",
    "    sections = [\n",
    "        'clinical_knowledge',\n",
    "        # 'college_medicine',\n",
    "    ]\n",
    "\n",
    "    mmlu_langs = [\n",
    "        'en',\n",
    "        'af',\n",
    "        'zu',\n",
    "        'xh',\n",
    "    ]\n",
    "\n",
    "    matrix = pd.DataFrame(\n",
    "        data=0.0,\n",
    "        index=[model_name],\n",
    "        columns=mmlu_langs\n",
    "    )\n",
    "\n",
    "    for lang in mmlu_langs:\n",
    "        total_score = 0\n",
    "        q_cnt = 0\n",
    "\n",
    "        for section in sections:\n",
    "\n",
    "            # Construct the pattern\n",
    "            pattern = re.compile(rf\".*-on-{lang}-mmlu-{section}.*\")\n",
    "\n",
    "            # Filter keys\n",
    "            matching_generations = [(c_id, gen) for c_id, gen in generations_map.items() if pattern.match(c_id)]\n",
    "            print(len(matching_generations))\n",
    "\n",
    "            for (c_id, gen) in matching_generations:\n",
    "                if check_mc_answer(c_id, gen):\n",
    "                    total_score += 1\n",
    "                q_cnt += 1\n",
    "\n",
    "        final_score = total_score / q_cnt\n",
    "        matrix.at[model_name, lang] = round(final_score*100, 1)\n",
    "\n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(12, 8), dpi=100)  # Increase the figure size and resolution for HD\n",
    "    ax = sns.heatmap(matrix, annot=matrix, cmap=\"Greens\", cbar=False, annot_kws={\"size\": 16}, fmt='.1f')\n",
    "\n",
    "    # Rotate the labels on the y-axis (left) to be horizontal\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=16)  # Increase y-axis label size\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), fontsize=16)  # Increase x-axis label size\n",
    "\n",
    "    # Display the heatmap\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    matrix.to_csv(f'mmlu_{model_name}_{i}.csv')\n",
    "\n",
    "    # Get and display Winogrande performance\n",
    "\n",
    "    winogrande_langs = [\n",
    "        'en',\n",
    "        'af',\n",
    "        'zu',\n",
    "        'xh',\n",
    "    ]\n",
    "\n",
    "    matrix = pd.DataFrame(\n",
    "        data=0.0,\n",
    "        index=[model_name],\n",
    "        columns=winogrande_langs\n",
    "    )\n",
    "\n",
    "    for lang in winogrande_langs:\n",
    "        total_score = 0\n",
    "        q_cnt = 0\n",
    "\n",
    "        # Construct the pattern\n",
    "        pattern = re.compile(rf\".*-on-{lang}-winogrande.*\")\n",
    "\n",
    "        # Filter keys\n",
    "        matching_generations = [(c_id, gen) for c_id, gen in generations_map.items() if pattern.match(c_id)]\n",
    "        print(len(matching_generations))\n",
    "\n",
    "        for (c_id, gen) in matching_generations:\n",
    "            if check_winogrande_answer(c_id, gen):\n",
    "                total_score += 1\n",
    "            q_cnt += 1\n",
    "\n",
    "        final_score = total_score / q_cnt\n",
    "        matrix.at[model_name, lang] = round(final_score*100, 1)\n",
    "\n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(12, 8), dpi=100)  # Increase the figure size and resolution for HD\n",
    "    ax = sns.heatmap(matrix, annot=matrix, cmap=\"Greens\", cbar=False, annot_kws={\"size\": 16}, fmt='.1f')\n",
    "\n",
    "    # Rotate the labels on the y-axis (left) to be horizontal\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=16)  # Increase y-axis label size\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), fontsize=16)  # Increase x-axis label size\n",
    "\n",
    "    # Display the heatmap\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    matrix.to_csv(f'winogrande_{model_name}_{i}.csv')\n",
    "\n",
    "    # Get and display Belebele performance\n",
    "\n",
    "    belebele_langs = [\n",
    "        'en',\n",
    "        'af',\n",
    "        'zu',\n",
    "        'xh',\n",
    "    ]\n",
    "\n",
    "    matrix = pd.DataFrame(\n",
    "        data=0.0,\n",
    "        index=[model_name],\n",
    "        columns=belebele_langs\n",
    "    )\n",
    "\n",
    "    for lang in belebele_langs:\n",
    "        total_score = 0\n",
    "        q_cnt = 0\n",
    "\n",
    "        # Construct the pattern\n",
    "        pattern = re.compile(rf\".*-on-{lang}-belebele.*\")\n",
    "\n",
    "        # Filter keys\n",
    "        matching_generations = [(c_id, gen) for c_id, gen in generations_map.items() if pattern.match(c_id)]\n",
    "        print(len(matching_generations))\n",
    "\n",
    "        for (c_id, gen) in matching_generations:\n",
    "            if check_mc_answer(c_id, gen):\n",
    "                total_score += 1\n",
    "            q_cnt += 1\n",
    "\n",
    "        final_score = total_score / q_cnt\n",
    "        matrix.at[model_name, lang] = round(final_score*100, 1)\n",
    "\n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(12, 8), dpi=100)  # Increase the figure size and resolution for HD\n",
    "    ax = sns.heatmap(matrix, annot=matrix, cmap=\"Greens\", cbar=False, annot_kws={\"size\": 16}, fmt='.1f')\n",
    "\n",
    "    # Rotate the labels on the y-axis (left) to be horizontal\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=16)  # Increase y-axis label size\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), fontsize=16)  # Increase x-axis label size\n",
    "\n",
    "    # Display the heatmap\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    matrix.to_csv(f'belebele_{model_name}_{i}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f8569c-7253-4ec6-8103-514b48a47673",
   "metadata": {
    "id": "b8f8569c-7253-4ec6-8103-514b48a47673"
   },
   "outputs": [],
   "source": [
    "# Skip running this cell if want to evaluate baseline\n",
    "# Save model\n",
    "model.push_to_hub_merged(model_name, tokenizer, save_method = \"merged_16bit\", token = hf_write_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fbb7a7-c3a7-4f45-8ed5-8456a21db5b8",
   "metadata": {
    "id": "40fbb7a7-c3a7-4f45-8ed5-8456a21db5b8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
